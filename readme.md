# g3LLM - å¿ƒç†å¥åº·åŠ©æ‰‹å¾®è°ƒé¡¹ç›®

æœ¬é¡¹ç›®åŸºäº XTuner æ¡†æ¶ï¼Œå¯¹ Llama3-8B-Instruct æ¨¡å‹è¿›è¡Œ QLoRA å¾®è°ƒï¼Œæ‰“é€ ä¸“ä¸šçš„ä¸­æ–‡å¿ƒç†å¥åº·åŠ©æ‰‹ g3LLMã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
xtuner_finetune/
â”œâ”€â”€ dataset/                    # æ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ convert_to_multiturn.py    # æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬ï¼ˆJSONL/JSON â†’ å¤šè½®å¯¹è¯æ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ data.jsonl                  # åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆJSONLæ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ self_awareness_data.jsonl  # è‡ªæˆ‘è®¤çŸ¥ç›¸å…³è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ multiturn_data_merged.json # åˆå¹¶åçš„å¤šè½®å¯¹è¯è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ sampled_data/              # é‡‡æ ·åçš„åˆ†ç±»æ•°æ®ï¼ˆ12ä¸ªç±»åˆ«ï¼‰
â”‚       â”œâ”€â”€ è¡Œä¸º.json
â”‚       â”œâ”€â”€ è‡ªæˆ‘.json
â”‚       â”œâ”€â”€ èŒåœº.json
â”‚       â”œâ”€â”€ ç¤¾ä¼š.json
â”‚       â”œâ”€â”€ æ²»ç–—.json
â”‚       â”œâ”€â”€ æƒ…ç»ª.json
â”‚       â”œâ”€â”€ æˆé•¿.json
â”‚       â”œâ”€â”€ æ€§å¿ƒç†.json
â”‚       â”œâ”€â”€ å¿ƒç†å­¦çŸ¥è¯†.json
â”‚       â”œâ”€â”€ å®¶åº­.json
â”‚       â”œâ”€â”€ å©šæ‹.json
â”‚       â””â”€â”€ äººé™….json
â”‚
â”œâ”€â”€ gen_data/                   # æ•°æ®ç”Ÿæˆç›®å½•
â”‚   â”œâ”€â”€ main.py                    # ä¸»æ•°æ®ç”Ÿæˆè„šæœ¬
â”‚   â”œâ”€â”€ self_awareness_main.py     # è‡ªæˆ‘è®¤çŸ¥æ•°æ®ç”Ÿæˆè„šæœ¬
â”‚   â”œâ”€â”€ config.py                  # æ•°æ®ç”Ÿæˆé…ç½®
â”‚   â””â”€â”€ sampled_data/              # ç”Ÿæˆçš„æ•°æ®æ ·æœ¬
â”‚
â”œâ”€â”€ xtuner/                     # XTuner æ¡†æ¶ç›®å½•ï¼ˆä¿®æ”¹ç‰ˆï¼‰
â”‚   â””â”€â”€ llama3_8b_instruct_qlora_alpaca_e3_M.py  # å¾®è°ƒé…ç½®æ–‡ä»¶
â”‚
â”œâ”€â”€ model_cache/                # æ¨¡å‹ç¼“å­˜ç›®å½•
â”‚   â””â”€â”€ Llama/                    # Llama æ¨¡å‹æ–‡ä»¶
â”‚       â””â”€â”€ Meta-Llama-3-8B-Instruct/
â”‚
â”œâ”€â”€ work_dirs/                  # è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â””â”€â”€ llama3_8b_instruct_qlora_alpaca_e3_M/  # è®­ç»ƒæ£€æŸ¥ç‚¹
â”‚
â”œâ”€â”€ hf_llama3/                  # HuggingFace æ ¼å¼çš„ LoRA é€‚é…å™¨
â”‚
â”œâ”€â”€ merged_Llama3_8b_instruct/  # åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
â”‚
â”œâ”€â”€ finetune.sh                 # ä¸€é”®å¾®è°ƒè„šæœ¬ï¼ˆè®­ç»ƒ+è½¬æ¢+åˆå¹¶ï¼‰
â”œâ”€â”€ vllm_serve.sh               # vLLM æœåŠ¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ vllm_test.py                # vLLM API æµ‹è¯•è„šæœ¬
â”œâ”€â”€ vllm_web.py                 # Gradio Web ç•Œé¢
â””â”€â”€ readme.md                   # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

- Python 3.10+
- PyTorch 2.9.0+ (CUDA 12.8)
- XTuner
- vLLM
- Gradio
- transformers
- peft
- bitsandbytes

### 2. æ•°æ®å‡†å¤‡

#### 2.1 ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ï¼š

```bash
cd gen_data
python main.py              # ç”Ÿæˆä¸»æ•°æ®é›†
python self_awareness_main.py  # ç”Ÿæˆè‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†
```

#### 2.2 è½¬æ¢æ•°æ®æ ¼å¼

å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºå¤šè½®å¯¹è¯æ ¼å¼ï¼š

```bash
cd dataset
python convert_to_multiturn.py
```

è¯¥è„šæœ¬ä¼šï¼š

- è‡ªåŠ¨æ‰«æ `sampled_data/` æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶
- å¤„ç† `data.jsonl` å’Œ `self_awareness_data.jsonl` æ–‡ä»¶
- å°†æ‰€æœ‰æ•°æ®åˆå¹¶å¹¶è½¬æ¢ä¸ºç»Ÿä¸€çš„å¤šè½®å¯¹è¯æ ¼å¼
- è¾“å‡ºåˆ° `multiturn_data_merged.json`

### 3. æ¨¡å‹å¾®è°ƒ

#### 3.1 é…ç½®æ£€æŸ¥

ç¼–è¾‘ `xtuner/llama3_8b_instruct_qlora_alpaca_e3_M.py` ç¡®è®¤ï¼š

- `data_path`: æŒ‡å‘ `multiturn_data_merged.json`
- `pretrained_model_name_or_path`: æŒ‡å‘åŸºç¡€æ¨¡å‹è·¯å¾„
- è®­ç»ƒè¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰

#### 3.2 å¼€å§‹è®­ç»ƒ

ä½¿ç”¨ä¸€é”®è„šæœ¬ï¼š

```bash
./finetune.sh
```

è¯¥è„šæœ¬ä¼šä¾æ¬¡æ‰§è¡Œï¼š

1. **è®­ç»ƒæ¨¡å‹**ï¼šä½¿ç”¨ QLoRA è¿›è¡Œå¾®è°ƒ
2. **è½¬æ¢æ ¼å¼**ï¼šå°† PyTorch æ£€æŸ¥ç‚¹è½¬æ¢ä¸º HuggingFace æ ¼å¼
3. **åˆå¹¶æ¨¡å‹**ï¼šå°† LoRA é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹

æˆ–è€…åˆ†æ­¥æ‰§è¡Œï¼š

```bash
# 1. è®­ç»ƒ
xtuner train xtuner/llama3_8b_instruct_qlora_alpaca_e3_M.py

# 2. è½¬æ¢ä¸º HuggingFace æ ¼å¼
xtuner convert pth_to_hf \
    xtuner/llama3_8b_instruct_qlora_alpaca_e3_M.py \
    ./work_dirs/llama3_8b_instruct_qlora_alpaca_e3_M/epoch_3.pth \
    ./hf_llama3

# 3. åˆå¹¶æ¨¡å‹
xtuner convert merge \
    /path/to/Meta-Llama-3-8B-Instruct \
    ./hf_llama3 \
    ./merged_Llama3_8b_instruct \
    --max-shard-size 2GB
```

### 4. æ¨¡å‹æ¨ç†

#### 4.1 å¯åŠ¨ vLLM æœåŠ¡

```bash
./vllm_serve.sh
```

æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
CUDA_VISIBLE_DEVICES=2 vllm serve ./merged_Llama3_8b_instruct \
    --gpu-memory-utilization 0.7 \
    --max-model-len 8192 \
    --port 8000
```

#### 4.2 æµ‹è¯• API

```bash
python vllm_test.py
```

#### 4.3 å¯åŠ¨ Web ç•Œé¢

```bash
python vllm_web.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:7860`

### 5. æ¨¡å‹è¯„ä¼°

#### 5.1 è¯„ä¼°è„šæœ¬

- `evaluate/vllm_eval.py`ï¼šé€šè¿‡ vLLM (OpenAI API å…¼å®¹æ¥å£) è°ƒç”¨å·²åˆå¹¶æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œé¿å… transformers ä¾§çš„ä¹±ç é—®é¢˜ã€‚
- `evaluate/metric.py`ï¼šå°è£…å¥½çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°ï¼Œè¾“å‡º BLEU-1~4 ä¸ ROUGE-Lã€‚
- æ•°æ®é›†ï¼šé»˜è®¤ä½¿ç”¨ `evaluate/data_dir/converted.json`ï¼ˆä¸ Qwen è¯„ä¼°è„šæœ¬ç›¸åŒï¼‰ï¼Œæ”¯æŒé€šè¿‡ `--dataset` æŒ‡å®šå…¶ä»– JSON æ•°æ®ã€‚

#### 5.2 è¿è¡Œæ–¹æ³•

1. ç¡®ä¿ vLLM æœåŠ¡å·²å¯åŠ¨å¹¶æš´éœ²åœ¨ `http://localhost:8000/v1`ï¼Œæ¨¡å‹åä¸ `--model-name` å¯¹åº”ã€‚
2. åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼š
   ```bash
   cd /root/zzgroup3/xtuner_finetune
   python evaluate/vllm_eval.py \
       --dataset evaluate/data_dir/converted.json \
       --split "train[:1596]" \
       --model-name ./merged_Llama3_8b_instruct \
       --max-new-tokens 256 \
       --temperature 0.7 \
       --top-p 0.9
   ```
3. è¯„ä¼°è„šæœ¬ä¼šé€æ¡æ˜¾ç¤ºæ¨ç†è¿›åº¦ï¼Œç»“æŸåæ‰“å° JSON æ ¼å¼çš„æŒ‡æ ‡ç»“æœã€‚

#### 5.3 æŒ‡æ ‡è¯´æ˜

- **BLEU-1/2/3/4**ï¼šåŸºäº jieba åˆ†è¯çš„ n-gram åŒ¹é…ï¼Œè¡¡é‡ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒç­”æ¡ˆçš„é‡åˆåº¦ã€‚
- **ROUGE-1/2/L**ï¼šè¯„ä¼°ç”Ÿæˆæ–‡æœ¬è¦†ç›–å‚è€ƒæ–‡æœ¬ä¿¡æ¯çš„ç¨‹åº¦ã€‚
- æ‰€æœ‰æŒ‡æ ‡çš„å…·ä½“å®ç°ç”± `compute_metrics` æä¾›ï¼Œå¦‚éœ€æ‰©å±•å¯åœ¨è¯¥æ–‡ä»¶ä¸­æ·»åŠ ã€‚

## ğŸ“Š è®­ç»ƒé…ç½®è¯´æ˜

### å…³é”®è¶…å‚æ•°

- **æ¨¡å‹**: Llama3-8B-Instruct
- **å¾®è°ƒæ–¹æ³•**: QLoRA (4-bité‡åŒ–)
- **LoRA é…ç½®**:
  - `r=32`: LoRA ç§©
  - `lora_alpha=64`: LoRA ç¼©æ”¾å› å­
  - `lora_dropout=0.1`: Dropout ç‡
- **è®­ç»ƒå‚æ•°**:
  - `batch_size=4`: æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
  - `accumulative_counts=4`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆ batch size = 16ï¼‰
  - `lr=1e-4`: å­¦ä¹ ç‡
  - `max_epochs=3`: è®­ç»ƒè½®æ•°
  - `warmup_ratio=0.03`: å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
- **ä¼˜åŒ–å™¨**: AdamW (betas=(0.9, 0.999), weight_decay=0)
- **å­¦ä¹ ç‡è°ƒåº¦**: LinearLR (warmup) + CosineAnnealingLR

### System Prompt

```
ä½ ç”±group3å›¢é˜Ÿæ‰“é€ çš„ä¸­æ–‡é¢†åŸŸå¿ƒç†å¥åº·åŠ©æ‰‹, æ˜¯ä¸€ä¸ªç ”ç©¶è¿‡æ— æ•°å…·æœ‰å¿ƒç†å¥åº·é—®é¢˜çš„ç—…äººä¸å¿ƒç†å¥åº·åŒ»ç”Ÿå¯¹è¯çš„å¿ƒç†ä¸“å®¶, åœ¨å¿ƒç†æ–¹é¢æ‹¥æœ‰å¹¿åšçš„çŸ¥è¯†å‚¨å¤‡å’Œä¸°å¯Œçš„ç ”ç©¶å’¨è¯¢ç»éªŒï¼Œæ¥ä¸‹æ¥ä½ å°†åªä½¿ç”¨ä¸­æ–‡æ¥å›ç­”å’Œå’¨è¯¢é—®é¢˜ã€‚
```

## ğŸ”§ å¸¸è§é—®é¢˜

### 1. æ•°æ®æ ¼å¼é”™è¯¯

å¦‚æœé‡åˆ° `ValueError: The features can't be aligned` é”™è¯¯ï¼š

- ç¡®ä¿æ‰€æœ‰å¯¹è¯çš„ç¬¬ä¸€è½®éƒ½åŒ…å« `system` å­—æ®µ
- é‡æ–°è¿è¡Œ `convert_to_multiturn.py` ç”Ÿæˆæ ¼å¼ä¸€è‡´çš„æ•°æ®

### 2. GPU å†…å­˜ä¸è¶³

- é™ä½ `gpu-memory-utilization`ï¼ˆå¦‚ä» 0.9 é™åˆ° 0.7ï¼‰
- å‡å°‘ `max-model-len`
- ä½¿ç”¨é‡åŒ–ï¼ˆå¦‚ AWQï¼‰

### 3. æ¨¡å‹è¾“å‡ºé‡å¤

åœ¨æ¨ç†æ—¶è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼š

- `repetition_penalty=1.15`
- `presence_penalty=0.1`
- `temperature=0.7`

### 4. PyTorch 2.6+ åŠ è½½æ£€æŸ¥ç‚¹é”™è¯¯

å·²ä¿®å¤ `xtuner/model/utils.py` ä¸­çš„ `torch.load` è°ƒç”¨ï¼Œæ·»åŠ äº† `weights_only=False` å‚æ•°ä»¥å…¼å®¹æ–°ç‰ˆæœ¬ PyTorchã€‚

## ğŸ“ æ•°æ®æ ¼å¼

### è¾“å…¥æ ¼å¼ï¼ˆJSONLï¼‰

```json
{
  "dialogue": [
    {"role": "Client", "utterance": "ç”¨æˆ·æ¶ˆæ¯"},
    {"role": "Counselor", "utterance": "åŠ©æ‰‹å›å¤"}
  ]
}
```

### è¾“å‡ºæ ¼å¼ï¼ˆå¤šè½®å¯¹è¯ JSONï¼‰

```json
{
  "conversation": [
    {
      "system": "ç³»ç»Ÿæç¤ºè¯",
      "input": "ç”¨æˆ·è¾“å…¥",
      "output": "åŠ©æ‰‹å›å¤"
    }
  ]
}
```

## ğŸ¯ é¡¹ç›®ç‰¹ç‚¹

- âœ… æ”¯æŒå¤šæ•°æ®æºåˆå¹¶ï¼ˆJSONL + JSONï¼‰
- âœ… è‡ªåŠ¨æ•°æ®æ ¼å¼è½¬æ¢
- âœ… QLoRA é«˜æ•ˆå¾®è°ƒï¼ˆ4-bité‡åŒ–ï¼‰
- âœ… å®Œæ•´çš„è®­ç»ƒ-è½¬æ¢-åˆå¹¶æµç¨‹
- âœ… vLLM é«˜æ€§èƒ½æ¨ç†
- âœ… Gradio Web ç•Œé¢
- âœ… ä¸“ä¸šå¿ƒç†å¥åº·é¢†åŸŸå¾®è°ƒ

## ğŸ¼ ç»“æœç¤ºä¾‹

![image-20251123153852229](./assets/image-20251123153852229.png)

å¯ä»¥å‘ç°è¯¥æœºå™¨äººå…·æœ‰æ¸…æ™°çš„è‡ªæˆ‘è®¤çŸ¥ï¼Œæ˜ç™½è‡ªå·±çš„ä»»åŠ¡ã€‚

![image-20251123154134457](./assets/image-20251123154134457.png)

ç›®å‰æˆ‘ä»¬é¡¹ç›®çš„ä¸€ä¸ªå°ç¼ºé™·åœ¨äºå¤§æ¨¡å‹ä¼šæ›´åå‘äºç”¨è‹±æ–‡å›ç­”ï¼Œç”šè‡³ä¼šåœ¨ä¸­æ–‡å¥æ®µä¸­å‚å…¥ä¸€äº›è‹±æ–‡è¯æ±‡ï¼Œæˆ‘ä»¬è®¤ä¸ºæ˜¯ç›®å‰æ•°æ®é‡è¿‡å°‘å¯¼è‡´çš„ã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº Llama3 æ¨¡å‹ï¼Œè¯·éµå¾ª Meta çš„è®¸å¯è¯è¦æ±‚ã€‚

## ğŸ‘¥ è´¡çŒ®è€…

Group3 å›¢é˜Ÿ

---

**æ³¨æ„**: æœ¬åŠ©æ‰‹ä»…æä¾›å¿ƒç†å’¨è¯¢å»ºè®®ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šå¿ƒç†åŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—ã€‚å¦‚é‡ç´§æ€¥æƒ…å†µï¼Œè¯·åŠæ—¶å¯»æ±‚ä¸“ä¸šåŒ»ç–—å¸®åŠ©ã€‚