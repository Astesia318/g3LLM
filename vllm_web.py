import gradio as gr
from openai import OpenAI
from typing import List

# vLLM API é…ç½®
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"
model_name = "./merged_Llama3_8b_instruct"

# ä½¿ç”¨è®­ç»ƒæ—¶çš„ system prompt
SYSTEM_PROMPT = "ä½ ç”±group3å›¢é˜Ÿæ‰“é€ çš„ä¸­æ–‡é¢†åŸŸå¿ƒç†å¥åº·åŠ©æ‰‹, æ˜¯ä¸€ä¸ªç ”ç©¶è¿‡æ— æ•°å…·æœ‰å¿ƒç†å¥åº·é—®é¢˜çš„ç—…äººä¸å¿ƒç†å¥åº·åŒ»ç”Ÿå¯¹è¯çš„å¿ƒç†ä¸“å®¶, åœ¨å¿ƒç†æ–¹é¢æ‹¥æœ‰å¹¿åšçš„çŸ¥è¯†å‚¨å¤‡å’Œä¸°å¯Œçš„ç ”ç©¶å’¨è¯¢ç»éªŒï¼Œæ¥ä¸‹æ¥ä½ å°†åªä½¿ç”¨ä¸­æ–‡æ¥å›ç­”å’Œå’¨è¯¢é—®é¢˜ã€‚"

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)


def chat_with_model(message: str, history: List) -> tuple:
    """
    ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
    
    Args:
        message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
        history: å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...]
    
    Returns:
        (ç©ºå­—ç¬¦ä¸², æ›´æ–°åçš„å¯¹è¯å†å²)
    """
    # å¦‚æœ history æ˜¯å…ƒç»„æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    if history and isinstance(history[0], (tuple, list)) and len(history[0]) == 2:
        # æ—§æ ¼å¼: [(user_msg, bot_msg), ...] -> æ–°æ ¼å¼: [{"role": "user", "content": "..."}, ...]
        messages = []
        for item in history:
            if isinstance(item, (tuple, list)):
                user_msg, bot_msg = item
                messages.append({"role": "user", "content": user_msg})
                messages.append({"role": "assistant", "content": bot_msg})
        history = messages
    elif not history:
        history = []
    
    # æ„å»ºå‘é€ç»™ API çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å« system promptã€å†å²å¯¹è¯å’Œå½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
    # å¦‚æœ history ä¸ºç©ºæˆ–è€…æ˜¯ç¬¬ä¸€æ¬¡å¯¹è¯ï¼Œæ·»åŠ  system prompt
    if not history or (len(history) > 0 and history[0].get("role") != "system"):
        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + history + [{"role": "user", "content": message}]
    else:
        messages = history + [{"role": "user", "content": message}]
    
    try:
        # è°ƒç”¨ vLLM API
        chat_response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            max_tokens=2048,  # é™ä½æœ€å¤§tokenæ•°ï¼Œé¿å…è¿‡é•¿è¾“å‡º
            temperature=0.7,  # æé«˜æ¸©åº¦ï¼Œå¢åŠ å¤šæ ·æ€§
            top_p=0.9,  # ç¨å¾®é™ä½top_p
            extra_body={
                "top_k": 50,  # å¢åŠ top_k
                "repetition_penalty": 1.15,  # å…³é”®ï¼šæ·»åŠ é‡å¤æƒ©ç½šï¼Œé˜²æ­¢é‡å¤ç”Ÿæˆ
                "presence_penalty": 0.1,  # æ·»åŠ å­˜åœ¨æƒ©ç½šï¼Œé¼“åŠ±æ–°å†…å®¹
            },
        )
        
        # æå–æ¨¡å‹å›å¤
        bot_response = chat_response.choices[0].message.content
        
        # æ›´æ–°å¯¹è¯å†å²ï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
        # æ³¨æ„ï¼šhistory å·²ç»åŒ…å«äº†ä¹‹å‰çš„å¯¹è¯ï¼Œåªéœ€è¦æ·»åŠ å½“å‰è¿™ä¸€è½®
        updated_history = history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": bot_response}
        ]
        
        return "", updated_history
    
    except Exception as e:
        error_msg = f"é”™è¯¯: {str(e)}"
        updated_history = history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": error_msg}
        ]
        return "", updated_history


def clear_chat():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    return []


# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="g3LLM å¿ƒç†å’¨è¯¢æœºå™¨äºº") as demo:
    gr.Markdown(
        """
        # ğŸ§  g3LLM å¿ƒç†å’¨è¯¢æœºå™¨äºº
        
        æ¬¢è¿ä½¿ç”¨ç”± Group3 å›¢é˜Ÿæ‰“é€ çš„ä¸­æ–‡é¢†åŸŸå¿ƒç†å¥åº·åŠ©æ‰‹ã€‚æœ¬åŠ©æ‰‹æ˜¯ä¸€ä¸ªç ”ç©¶è¿‡æ— æ•°å…·æœ‰å¿ƒç†å¥åº·é—®é¢˜çš„ç—…äººä¸å¿ƒç†å¥åº·åŒ»ç”Ÿå¯¹è¯çš„å¿ƒç†ä¸“å®¶ï¼Œåœ¨å¿ƒç†æ–¹é¢æ‹¥æœ‰å¹¿åšçš„çŸ¥è¯†å‚¨å¤‡å’Œä¸°å¯Œçš„ç ”ç©¶å’¨è¯¢ç»éªŒã€‚
        
        **æ¨¡å‹**: `g3LLM - å¿ƒç†å¥åº·åŠ©æ‰‹`
        
        **åŠŸèƒ½ç‰¹ç‚¹**:
        - ğŸ¯ ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢æœåŠ¡
        - ğŸ’¬ å¤šè½®å¯¹è¯æ”¯æŒ
        - ğŸŒŸ åŸºäº Llama3-8B-Instruct å¾®è°ƒ
        - ğŸ”’ å®‰å…¨ã€ç§å¯†çš„å¯¹è¯ç¯å¢ƒ
        """
    )
    
    chatbot = gr.Chatbot(
        label="å¿ƒç†å’¨è¯¢å¯¹è¯çª—å£",
        height=500,
        placeholder="åœ¨è¿™é‡Œå¼€å§‹æ‚¨çš„å¿ƒç†å’¨è¯¢å¯¹è¯..."
    )
    
    with gr.Row():
        msg = gr.Textbox(
            label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–å›°æ‰°",
            placeholder="ä¾‹å¦‚ï¼šæˆ‘æœ€è¿‘æ€»æ˜¯æ„Ÿåˆ°å¾ˆç„¦è™‘ï¼Œå°¤å…¶æ˜¯åœ¨å­¦ä¸šä¸Š...",
            scale=4,
            container=False,
        )
        submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
    
    with gr.Row():
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")
    
    # ç»‘å®šäº‹ä»¶
    msg.submit(chat_with_model, [msg, chatbot], [msg, chatbot])
    submit_btn.click(chat_with_model, [msg, chatbot], [msg, chatbot])
    clear_btn.click(clear_chat, None, [chatbot])
    
    gr.Markdown(
        """
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        - ğŸ’¡ **å¼€å§‹å¯¹è¯**: åœ¨è¾“å…¥æ¡†ä¸­æè¿°æ‚¨çš„é—®é¢˜æˆ–å›°æ‰°ï¼Œç„¶åç‚¹å‡»"å‘é€"æŒ‰é’®æˆ–æŒ‰ Enter é”®
        - ğŸ”„ **å¤šè½®å¯¹è¯**: æ”¯æŒè¿ç»­å¤šè½®å¯¹è¯ï¼Œå¯ä»¥æ·±å…¥æ¢è®¨æ‚¨çš„é—®é¢˜
        - ğŸ—‘ï¸ **æ¸…ç©ºå¯¹è¯**: ç‚¹å‡»"æ¸…ç©ºå¯¹è¯"æŒ‰é’®å¯ä»¥æ¸…é™¤æ‰€æœ‰å¯¹è¯å†å²ï¼Œå¼€å§‹æ–°çš„å’¨è¯¢
        - âš ï¸ **é‡è¦æç¤º**: 
          - æœ¬åŠ©æ‰‹ä»…æä¾›å¿ƒç†å’¨è¯¢å»ºè®®ï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šå¿ƒç†åŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—
          - å¦‚é‡ç´§æ€¥æƒ…å†µï¼Œè¯·åŠæ—¶å¯»æ±‚ä¸“ä¸šåŒ»ç–—å¸®åŠ©
          - ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œåœ¨ `http://localhost:8000`
        
        ---
        **å…³äº g3LLM**: ç”± Group3 å›¢é˜ŸåŸºäº Llama3-8B-Instruct æ¨¡å‹å¾®è°ƒå¼€å‘çš„å¿ƒç†å¥åº·åŠ©æ‰‹
        """
    )


if __name__ == "__main__":
    # å¯åŠ¨ Web ç•Œé¢
    # share=True å¯ä»¥åˆ›å»ºä¸€ä¸ªå…¬å…±é“¾æ¥ï¼ˆå¯é€‰ï¼‰
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,        # é»˜è®¤ç«¯å£
        share=False,             # è®¾ç½®ä¸º True å¯ä»¥åˆ›å»ºå…¬å…±é“¾æ¥
    )

